{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "#PROYECTO FINAL\n",
        "#MINERIA DE DATOS\n",
        "#CLASIFICADOR DE IMAGENES\n",
        "##Tony Chamorro Aguilar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otmJgmbahf"
      },
      "source": [
        "#Introduccion.\n",
        "Los modelos de clasificacion de imagenes tienen millones de parametros. Entrenarlos desde cero requiere una gran cantidad de datos de entrenamiento etiquetados y mucha potencia informática.\n",
        "\n",
        " El aprendizaje por transferencia es una técnica que ataja gran parte de esto al tomar una parte de un modelo que ya ha sido perturbado en una tarea relacionada y reutilizarlo en un nuevo modelo.\n",
        "\n",
        "¿Cómo procesar imágenes con machine learning para reconocimiento y clasificación de objetos? La identificación de objetos en imágenes tiene múltiples aplicaciones: desde algo tan prosaico como identificar gatos o perros en fotografías, hasta la detección de tumores en pruebas diagnósticas o clasificar las piezas de una línea de producción según su calidad. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo del Proyecto.\n",
        "Lo que se busca es entrenar un modelo con una serie de datos en imagenes.Algunas tareas de clasificación que son difíciles para los humanos resultan fáciles para redes neuronales. Usando un simple procedimiento de ajuste fino, una red neuronal puede ayudar.\n",
        "\n",
        "#Datos a usar.\n",
        "Inicialmente se va a trabajar con unos datos extraidos de una pagina https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\n",
        "\n",
        "\n",
        "#Algoritmos a utilizar.\n",
        "Aprendizaje por transferencia demuestra cómo crear un modelo de Keras para clasificar cinco especies de flores mediante el uso de un modelo guardado TF2 previamente perturbado de TensorFlow Hub para la extracción de características de imagenes.\n",
        "\n",
        "Keras es una API de aprendizaje profundo escrita en Python, que se ejecuta sobre la plataforma de aprendizaje automático TensorFlow . Fue desarrollado con un enfoque en permitir la experimentación rápida y proporcionar una experiencia de desarrollador agradable.\n",
        "\n",
        "El propósito de Keras es brindar una ventaja injusta a cualquier desarrollador que busque enviar aplicaciones basadas en ML.\n",
        "\n",
        "Keras es:\n",
        "\n",
        "*   Simple , pero no simplista. Keras reduce la carga cognitiva del\n",
        "desarrollador para que pueda concentrarse en las partes del problema que realmente importan. Keras se centra en la facilidad de uso, la velocidad de depuración, la elegancia y la concisión del código, la capacidad de mantenimiento y la capacidad de implementación (a través de TFServing, TFLite, TF.js).\n",
        "\n",
        "*   Flexible : Keras adopta el principio de revelación progresiva de la \n",
        "complejidad : los flujos de trabajo simples deben ser rápidos y fáciles, mientras que los flujos de trabajo arbitrariamente avanzados deben ser posibles a través de un camino claro que se basa en lo que ya ha aprendido.\n",
        "\n",
        "\n",
        "*   Potente : Keras proporciona un rendimiento y una escalabilidad sólidos en la industria: lo utilizan organizaciones y empresas, incluidas la NASA, YouTube y Waymo. Así es: sus recomendaciones de YouTube cuentan con la tecnología de Keras, al igual que el vehículo sin conductor más avanzado del mundo.\n"
      ],
      "metadata": {
        "id": "wU5CZZHFq4E5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Desarrolla el modelo Keras desde cero"
      ],
      "metadata": {
        "id": "dehNLgdouS7I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "Importa bibliotecas y define constantes.\n",
        "\n",
        "Primero, importa las bibliotecas de Python necesarias para el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlauq-4FWGZM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "Carga la direccion con el directorio de trabajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlsEcKVeuCnf"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "model_name = \"efficientnetv2-xl-21k\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "##Configurar el conjunto de datos de Flowers\n",
        "Las entradas se redimensionan adecuadamente para el módulo seleccionado. El aumento del conjunto de datos (es decir, distorsiones aleatorias de una imagen cada vez que se lee) mejora el entrenamiento, especialmente. al realizar un ajuste fino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBtFK1hO8KsO"
      },
      "outputs": [],
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umB5tswsfTEQ"
      },
      "outputs": [],
      "source": [
        "def build_dataset(subset):\n",
        "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_dir,\n",
        "      validation_split=.20,\n",
        "      subset=subset,\n",
        "      label_mode=\"categorical\",\n",
        "      # Seed needs to provided when using validation_split and shuffle = True.\n",
        "      # A fixed seed is used so that the validation set is stable across runs.\n",
        "      seed=123,\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=1)\n",
        "\n",
        "train_ds = build_dataset(\"training\")\n",
        "class_names = tuple(train_ds.class_names)\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "train_ds = train_ds.repeat()\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "do_data_augmentation = True #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomRotation(40))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
        "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
        "  # If all training inputs are larger than image_size, one could also use\n",
        "  # RandomCrop with a batch size of 1 and rebatch later.\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "val_ds = build_dataset(\"validation\")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "# Definiendo el modelo\n",
        "Todo lo que necesita es poner un clasificador lineal en la parte superior de la feature_extractor_layercon el módulo de concentradores.\n",
        "\n",
        "Para la velocidad, comenzamos con un no entrenable feature_extractor_layer, pero también se puede permitir el ajuste fino para una mayor precisión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50FYNIb1dmJH"
      },
      "outputs": [],
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Entrenando el modelo.\n",
        "\n",
        "A continuación, crea estos conjuntos de datos de entrenamiento y evaluación. Usa los hiperparámetros para establecer cómo el conjunto de datos de entrenamiento le proporcionará ejemplos al modelo durante el entrenamiento. \n",
        "\n",
        "Establece el conjunto de datos de validación a fin de brindar todos sus ejemplos en un solo lote, para un solo paso de validación al final de cada ciclo de entrenamiento.\n",
        "\n",
        "Crea conjuntos de datos de entrenamiento y validación.\n",
        "\n",
        "Crea una función de entrada para convertir características y etiquetas  para entrenamiento o evaluación:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El progreso del entrenamiento puede verse de la siguiente manera:"
      ],
      "metadata": {
        "id": "R6L8-cw9xjfY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_YKX2Qnfg6x"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYOw0fTO1W4x"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8DKKgeKv4-"
      },
      "source": [
        "Pruebe el modelo en una imagen de los datos de validacion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi1iCNB9K1Ai"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(val_ds))\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejemplo extra\n",
        "#Clasificación de imágenes de frutas de CNN desde cero"
      ],
      "metadata": {
        "id": "oaa835q9OYA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install opendatasets for download kaggle dataset into colab notebook\n",
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "DEPoEfTmO4si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import opendatasets as od\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "Nqbe7-GSPMQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "od.download(\"https://www.kaggle.com/datasets/moltean/fruits\")"
      ],
      "metadata": {
        "id": "rKVHvlAIPPtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saltando, encontró archivos descargados en \"./frutas\" (use force=True para forzar la descarga)"
      ],
      "metadata": {
        "id": "nUTX6CvCPjGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Colab GPU and checking it\n",
        "print(\"GPU is connected...\" if tf.config.list_physical_devices(\"GPU\") else \"Please check your runtime and run it again\")"
      ],
      "metadata": {
        "id": "OdftSyitPVDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparando imagenes\n",
        "\n"
      ],
      "metadata": {
        "id": "_86Z1OebP9G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test filepath\n",
        "train_path = \"/content/fruits/fruits-360_dataset/fruits-360/Training/\"\n",
        "test_path = \"/content/fruits/fruits-360_dataset/fruits-360/Test/\""
      ],
      "metadata": {
        "id": "LEhG7DybQFdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We make fruits more legible so that we can examine them better\n",
        "labels = []\n",
        "filenames = []\n",
        "\n",
        "for i in os.listdir(train_path):\n",
        "  for image_filename in os.listdir(train_path + i):\n",
        "    labels.append(i)\n",
        "    filenames.append(train_path + i + \"/\" + image_filename) # Image of the fruit"
      ],
      "metadata": {
        "id": "9CHTUho0QI5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First 100 of labels\n",
        "list(labels[:100])"
      ],
      "metadata": {
        "id": "8oYJAHNgQMWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames[10:20]"
      ],
      "metadata": {
        "id": "T5zaCa4cQP_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fruits DataFrame\n",
        "fruit_data = pd.DataFrame(\n",
        "    labels, columns=[\"Classes\"]\n",
        ")\n",
        "fruit_data[\"Filenames\"] = filenames"
      ],
      "metadata": {
        "id": "VQ1883UsQm66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_data"
      ],
      "metadata": {
        "id": "RCALsmrKQsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.histplot(data=fruit_data, x=fruit_data[\"Classes\"].value_counts(), kde=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9IbNSrqQzSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create graph of fruit images with \n",
        "import plotly.express as px\n",
        "fig = px.histogram(fruit_data, x=\"Classes\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "htGYMvtHQ4FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.unique(labels))"
      ],
      "metadata": {
        "id": "N3QlXZNJQ7CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image with keras.utils\n",
        "from tensorflow.keras.utils import load_img\n",
        "img = load_img(filenames[1])\n",
        "plt.imshow(img)\n",
        "plt.title(labels[1])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uOWPyG73Q8fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image with IPython \n",
        "from IPython.display import Image\n",
        "Image(filenames[3000])"
      ],
      "metadata": {
        "id": "nuWTlDNbREOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convertir etiquetas de datos en tipo booleano\n",
        "\n"
      ],
      "metadata": {
        "id": "PuUVHLaaRMod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya que tenemos la ruta de archivo de las imágenes de nuestro conjunto de datos de tren en una lista, y vamos a convertir nuestras etiquetas en tipo booleano"
      ],
      "metadata": {
        "id": "7qSb7oCXRfua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have already hase images labels in a list, now find unique values and save them into variable\n",
        "\n",
        "unique_fruits = np.unique(labels)\n",
        "unique_fruits"
      ],
      "metadata": {
        "id": "m8sTyufuRFYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's turn a single label into boolean type\n",
        "print(labels[1])\n",
        "labels[1] == unique_fruits\n",
        "\n",
        "# As you can see label[1] value returned true in unique_fruits where it located"
      ],
      "metadata": {
        "id": "QlPcqgPiRq7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn all labels into boolean dtype \n",
        "boolean_labels = [label == unique_fruits for label in labels]\n",
        "\n",
        "# Compare length of labels and boolean_labels\n",
        "print(f\"Labels: {len(labels)}\")\n",
        "print(f\"Boolean labels: {len(boolean_labels)}\")"
      ],
      "metadata": {
        "id": "piuyCjNVRvkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crear nuestro propio conjunto de datos de validación"
      ],
      "metadata": {
        "id": "heMWEH8FR2jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = filenames\n",
        "y = boolean_labels"
      ],
      "metadata": {
        "id": "2pyZ2mdpR6UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)\n"
      ],
      "metadata": {
        "id": "15e38bRrR_uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid,  y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X_train), len(y_train)"
      ],
      "metadata": {
        "id": "Gup7Gq-mSC9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_valid), len(y_valid)"
      ],
      "metadata": {
        "id": "z9yC45LeSH7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imágenes de precesión\n",
        "Convertir imágenes en tensores\n",
        "\n",
        "Cree una función con los siguientes pasos:\n",
        "\n",
        "Tomar la ruta del archivo de imagen como entrada\n",
        "Use Tensorflow para leer un archivo y guardarlo en una imagen variable\n",
        "Convierte nuestras imágenes en Tensores\n",
        "Normalizar nuestra imagen (Convertir valores de color de 1-255 a 0-1)\n",
        "Cambiar el tamaño de las imágenes para que sean una forma de (224,224)\n",
        "Volver a la imagen modificada\n"
      ],
      "metadata": {
        "id": "D5zUL7PBSU_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup image size\n",
        "IMG_SIZE = 224\n",
        "# Create a function for preprocessing images\n",
        "def process_image(file_path, image_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Take image filepath and turn it into Tensors \n",
        "  \"\"\"\n",
        "  # Read image file and save into variable\n",
        "  image = tf.io.read_file(file_path)\n",
        "  # Turn images into Tensors with 3 color channels\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # Normalize color channel values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize images\n",
        "  image = tf.image.resize(image, size=(IMG_SIZE,IMG_SIZE))\n",
        "  # Return images\n",
        "  return image"
      ],
      "metadata": {
        "id": "jcRHq5h7SLp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Convierta los datos en lotes"
      ],
      "metadata": {
        "id": "9botgBoCSvBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué usamos lotes en el aprendizaje profundo?\n",
        "\n",
        "Otra razón por la que debería considerar usar lote es que cuando entrena su modelo de aprendizaje profundo sin dividirlo en lotes, entonces su algoritmo de aprendizaje profundo (puede ser una red neuronal) tiene que almacenar valores de error para todas esas 100000 imágenes en la memoria y esto causará una gran disminución en la velocidad de entrenamiento"
      ],
      "metadata": {
        "id": "GdUYB3_yS6fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for preprocess images and return to tuple of (image, label)\n",
        "\n",
        "def get_image_label(filepath, label):\n",
        "  \"\"\"\n",
        "  Get image filepath and preprocess and returns a tuple of image, label\n",
        "  \"\"\"\n",
        "  image = process_image(filepath)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "zjZvnsHjTcJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes we classify more images during process. More media data can not run very well on device.\n",
        "# So that we need create data batches\n",
        "# Data batches contain images which we defined image size\n",
        "\n",
        "# Setup batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create a function for turn our images into batches\n",
        "def create_data_batches(X, y=None,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        valid_data=False,\n",
        "                        test_data = False):\n",
        "  \"\"\"\n",
        "  This function creates batches of data out of images(X), and (y) pairs.\n",
        "  Shuffle the data if it will be training dataset, however doesn't shuffle validation dataset\n",
        "  Also accepts test data as input but no labels\n",
        "  \"\"\"\n",
        "  if test_data:\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "  \n",
        "  elif valid_data:\n",
        "    data = tf.data.Dataset.from_tensor_slices((\n",
        "        tf.constant(X),\n",
        "        tf.constant(y)\n",
        "    ))\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "  \n",
        "  else:\n",
        "    data = tf.data.Dataset.from_tensor_slices((\n",
        "        tf.constant(X),\n",
        "        tf.constant(y),\n",
        "    ))\n",
        "    data = data.shuffle(buffer_size = len(X))\n",
        "    data = data.map(get_image_label)\n",
        "    data_batch = data.batch(BATCH_SIZE)\n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "wp-nmtvJTjz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and validartion data batches\n",
        "train_data = create_data_batches(X_train, y_train)\n",
        "valid_data = create_data_batches(X_valid, y_valid, valid_data=True)"
      ],
      "metadata": {
        "id": "avLQprQkTwe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "QAYoiRE0T35A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_data)"
      ],
      "metadata": {
        "id": "xqKhICU7T9dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizacion data"
      ],
      "metadata": {
        "id": "WPfYTi7mUDS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for viewing images in a data batches\n",
        "\n",
        "def show_images(images, labels):\n",
        "  \"\"\"\n",
        "  Display 30 images and with their labels from data baches\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup figure\n",
        "  plt.figure(figsize=(10,10))\n",
        "  # Loop through 30 images\n",
        "  for i in range(30):\n",
        "    # Create subplots(5rows, 6columns)\n",
        "    ax = plt.subplot(5,6, i+1)\n",
        "    # Display images\n",
        "    plt.imshow(images[i])\n",
        "    # Add image label as a title\n",
        "    plt.title(unique_fruits[labels[i].argmax()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "tAJFNhwZU2IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, train_labels = next(train_data.as_numpy_iterator())\n",
        "len(train_images), len(train_labels)"
      ],
      "metadata": {
        "id": "1-tfVEpFVYRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(train_images, train_labels)"
      ],
      "metadata": {
        "id": "5-gy7govVi7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Preparar entradas y salidas."
      ],
      "metadata": {
        "id": "fOWx2aTtVuuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape to the model\n",
        "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # // None is batch size\n",
        "\n",
        "# Setup output shape for model\n",
        "OUTPUT_SHAPE = len(unique_fruits)\n",
        "\n",
        "# Get pre trained model url(Tensorflow Hub)\n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5\""
      ],
      "metadata": {
        "id": "4SNB0JonV3T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelado\n"
      ],
      "metadata": {
        "id": "kgRGa6uGV8N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una función con los siguientes pasos:\n",
        "\n",
        "Toma la forma de entrada, la forma de salida y la URL del modelo, que hemos definido\n",
        "\n",
        "Define las capas en un modelo de Keras de forma secuencial\n",
        "\n",
        "Compile el modelo (dice que debe ser evaluado y mejorado)\n",
        "\n",
        "Construya el modelo (le dice al moodel la forma de entrada que obtendrá)\n",
        "\n",
        "\n",
        "1.   Devolver el modelo\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vuaKJRbzWHT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url = MODEL_URL):\n",
        "  print(\"Now, building model for you project: \")\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.KerasLayer(MODEL_URL), # // Layers input\n",
        "      tf.keras.layers.Dense(units=OUTPUT_SHAPE, activation=\"softmax\"), # //Layer for output\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "      optimizer = tf.keras.optimizers.Adam(),\n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  # Build model\n",
        "  model.build(INPUT_SHAPE)\n",
        "  print(\"Model has been built successfully!\")\n",
        "  # Return the model\n",
        "  return model"
      ],
      "metadata": {
        "id": "gHXjYKz3Wo0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a function for viewing images in a data batches\n",
        "\n",
        "def show_images(images, labels):\n",
        "  \"\"\"\n",
        "  Display 30 images and with their labels from data baches\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup figure\n",
        "  plt.figure(figsize=(10,10))\n",
        "  # Loop through 30 images\n",
        "  for i in range(30):\n",
        "    # Create subplots(5rows, 6columns)\n",
        "    ax = plt.subplot(5,6, i+1)\n",
        "    # Display images\n",
        "    plt.imshow(images[i])\n",
        "    # Add image label as a title\n",
        "    plt.title(unique_fruits[labels[i].argmax()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "dcdnwiN7UPab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Create a function for viewing images in a data batches\n",
        "def show_images(images, labels): \"\"\" Display 30 images and with their labels from data baches \"\"\"\n",
        "\n",
        "Setup figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "Loop through 30 images\n",
        "for i in range(30):\n",
        "\n",
        "# Create subplots(5rows, 6columns)\n",
        "ax = plt.subplot(5,6, i+1)\n",
        "# Display images\n",
        "plt.imshow(images[i])\n",
        "# Add image label as a title\n",
        "plt.title(unique_fruits[labels[i].argmax()])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "POYUHIpWUCw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONCLUSIONES\n",
        "Keras es un marco altamente flexible adecuado para iterar sobre ideas de investigación de vanguardia. \n",
        "\n",
        "Sigue el principio de divulgación progresiva de la complejidad : facilita el inicio, pero permite manejar casos de uso arbitrariamente avanzados, que solo requieren un aprendizaje incremental en cada paso.\n",
        "\n",
        "De la misma manera que pudo entrenar y evaluar una red neuronal simple arriba en unas pocas líneas, puede usar Keras para desarrollar rápidamente nuevos procedimientos de entrenamiento o arquitecturas de modelos exóticos. "
      ],
      "metadata": {
        "id": "NIcU3H05yKmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E7duZSzmT8Hz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ],
      "name": "TF Hub for TF2: Retraining an image classifier",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}